{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9685b695",
   "metadata": {},
   "source": [
    "# Part 1: \n",
    "### a: In your opinion, what were the most important turning points in the history of deep learning?\n",
    "In my opinion, the history of deep learning has several key turning points. Early neural networks and perceptrons introduced the idea of learning from data, but training deep networks became practical only after backpropagation was developed. The 2006 paper on deep belief networks and the term \"deep learning\" marked a significant turning point, separating it from traditional neural networks.\n",
    "\n",
    "The success of GPU-accelerated CNNs in 2012 (e.g., AlexNet) showed that deep networks could achieve state-of-the-art results in image recognition. Later, architectures like RNNs, LSTMs, and Transformers revolutionized natural language processing and sequence modeling. Today, large language models such as ChatGPT have transformed how we interact with AI. Personally, I found OpenAI Five impressive as well, since it showed how reinforcement learning could train agents to reach a superhuman win rate in complex environments.\n",
    "\n",
    "### b: Explain the ADAM optimizer.\n",
    "The ADAM (Adaptive Moment Estimation) is basically an optimizer that improves how neural networks learn. It is one of the most widely used optimizers in deep learning. ADAM optimizer is an advanced optimization algorithm that combines the benefits of two other popular optimizers: AdaGrad (moving averages of past gradients) and RMSProp (scaling updates based on squared gradients). It computes adaptive learning rates for each parameter by maintaining two moving averages: the first moment (mean) and the second moment (uncentered variance) of the gradients. This means it can train faster and more reliably on large or noisy datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92818090",
   "metadata": {},
   "source": [
    "### c: Assume data input is a single 30x40 pixel image. First layer is a convolutional layer with 5 filters, with kernel size 3x2, step size (1,1) and padding='valid'. What are the output dimensions?\n",
    "\n",
    "\n",
    "We use the standard convolution output size formula:\n",
    "\n",
    "[\n",
    "H_{out} = \\frac{H_{in} + 2 \\cdot P_h - K_h}{S_h} + 1\n",
    "]\n",
    "[\n",
    "W_{out} = \\frac{W_{in} + 2 \\cdot P_w - K_w}{S_w} + 1\n",
    "]\n",
    "\n",
    "Where:\n",
    "\n",
    "* (H_{in}, W_{in}) = input height and width\n",
    "* (K_h, K_w) = kernel size\n",
    "* (S_h, S_w) = stride\n",
    "* (P_h, P_w) = padding (0 for `\"valid\"`)\n",
    "\n",
    "Now in Python:\n",
    "\n",
    "```python\n",
    "# Part 1 - c\n",
    "# Input dimensions\n",
    "H_in, W_in = 30, 40  \n",
    "\n",
    "# Conv layer parameters\n",
    "kernel_h, kernel_w = 3, 2\n",
    "stride_h, stride_w = 1, 1\n",
    "pad_h, pad_w = 0, 0   # 'valid' means no padding\n",
    "num_filters = 5\n",
    "\n",
    "# Compute output dimensions\n",
    "H_out = (H_in + 2*pad_h - kernel_h)//stride_h + 1\n",
    "W_out = (W_in + 2*pad_w - kernel_w)//stride_w + 1\n",
    "\n",
    "output_shape = (num_filters, H_out, W_out)\n",
    "print(\"Output shape:\", output_shape)\n",
    "```\n",
    "\n",
    "✅ **Expected result:**\n",
    "\n",
    "```\n",
    "Output shape: (5, 28, 39)\n",
    "```\n",
    "\n",
    "So the output is **5 feature maps of size 28×39**.\n",
    "\n",
    "---\n",
    "\n",
    "Want me to also format the **answer explanation** (like part a and b) so it reads like: *“The output dimensions are (5, 28, 39) because …”*?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4490e3a8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
